\section{既往研究の整理}\label{sec:researches}
本章では，本研究に関係する既往研究を整理する．\par
\subsection{用語および記号の定義}\label{subsec:definition}

ここでは本論文で用いる用語と記号を定義する．

まず，本論文における自己位置とは，センサの３次元座標$\mathbf{t}={
	(t_{(x)} , 
	t_{(y)} ,
	t_{(z)})
}^\mathrm{T}\in\mathbb{R}^3$に加えて，姿勢の回転方向を含む，計6つの自由度を持った情報である．センサの姿勢は，$z,y,x$軸周りの回転である３つのオイラー角$\alpha, \beta, \gamma$の組み合わせ以外にも，次の回転行列$\mathbf{R}$で表すこともでき，本論文では後者を採用する．
\begin{equation}
	\mathbf{R}=
	\mathbf{R}_{z}(\alpha) \mathbf{R}_{y}(\beta) \mathbf{R}_{x}(\gamma)=
	\left[\begin{array}{ccc}\cos \alpha & -\sin \alpha & 0 \\ \sin \alpha & \cos \alpha & 0 \\ 0 & 0 & 1\end{array}\right]
	\left[\begin{array}{ccc}\cos \beta & 0 & \sin \beta \\ 0 & 1 & 0 \\ -\sin \beta & 0 & \cos \beta\end{array}\right]
	\left[\begin{array}{ccc}1 & 0 & 0 \\ 0 & \cos \gamma & -\sin \gamma \\ 0 & \sin \gamma & \cos \gamma\end{array}\right]
\end{equation}
\par
続いて，空間座標系を自己位置$(\mathbf{t},  \mathbf{R})$に応じて定義する．時刻$t=l$の自己位置によって定まる空間座標系（以後，座標系$t=l$と呼ぶ）上での時刻$t=k$の自己位置を$(\mathbf{t}^{(l)}_k,  \mathbf{R}^{(l)}_k)$として，
\begin{equation}
	\mathbf{t}^{(k)}_k=\left(\begin{array}{c}
		0\\0\\0
	\end{array}\right), \,  
	\mathbf{R}^{(k)}_k=\left[\begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{array}\right]
\end{equation}
となるように空間座標系を定義する．以上の条件のもと，センサがカメラである場合には特に，投影平面の右方向に$x$軸，下方向に$y$軸，投影平面を垂直に奥向きに貫く方向を$z$軸とする．
この時，座標系$t=k$上の点
$\mathbf{m}^{(k)}={(x^{(k)} , y^{(k)}, z^{(k)})}^\mathrm{T}$は
,同次座標表現された位置ベクトル$\tilde{\mathbf{m}}^{(k)}={(x^{(k)} , y^{(k)}, z^{(k)},1)}^\mathrm{T}$を用いて座標系$t=l$上で
\begin{equation}\label{eq:trans}
	\tilde{\mathbf{m}}^{(l)}
	=
	\left(\begin{array}{c}
		x^{(l)}\\ y^{(l)}\\ z^{(l)} \\ 1
	\end{array}
	\right)
	=
	\left[\begin{array}{cc}
		\mathbf{R}^{(l)}_k & \mathbf{t}^{(l)}_k \\ \mathbf{0} & 1
	\end{array}
	\right]
	\tilde{\mathbf{m}}^{(k)}
\end{equation}
と表現できる．
以後，特に座標系$t=0$を絶対座標系とし，それに従い$\mathbf{m}^{(0)}$を絶対座標，$\mathbf{R}^{(0)}_k,\, \mathbf{t}^{(0)}_k$をまとめて絶対自己位置または単に自己位置と呼ぶ．
%また，座標系$t=l$での時刻$t=k$の自己位置$(\mathbf{t}^{(l)}_k,  \mathbf{R}^{(l)}_k)$を座標系$t=l$に対する時刻$t=k$の相対自己位置と呼ぶ．
\par

\subsection{要素技術}\label{subsec:techniques}

本研究で提案手法，および比較手法を構築するために用いられる画像処理・推定技術を紹介する．
具体的には，ビデオ・セグメンテーション，オプティカルフローの推定，単眼深度推定である．
これらの技術は近年の機械学習の著しい進歩を背景に，深層学習に基づく推定手法が精度・速度の面で高い成果を上げている．

\subsubsection{ビデオ・セグメンテーション}\label{subsec:video_segmentation}

セグメンテーションとは画像に映り込んだ物体の画素領域を分離する技術であり，コンピュータービジョン分野におけるトピックの一つである．
セグメンテーションの出力の一つとして得られる，注目する物体と他の部分において画素値が異なる画像は，マスク画像と呼ばれる．
その中でもビデオ・セグメンテーションとは，動画を入力としてセグメンテーションを行うタスクである．
一枚の画像をセグメンテーションするイメージ・セグメンテーションと異なり，ビデオ・セグメンテーションはフレーム間の物体の対応関係が与えられるという点で，画像内での物体の追跡を包含している．
物体同士の対応については，コーナーやエッジ，輪郭といった部分的な特徴を持った点（以下，特徴点）を抽出し，それらをもとに探索・追跡が行われ，物体が対応付けられる．

ビデオ・セグメンテーションはVideo Object Segmentation(以下，VOS)とVideo Semantic Segmentation(以下，VSS)に分かれる．
VOSとは動画内の何かしらの物体をカテゴリの指定なく抽出するタスクである．
セグメンテーションされた物体が何であるかという物体認識は含まれない．
VSSとはあらかじめ定義されていたカテゴリ分類に基づいて物体を抽出するタスクである．
歩行者や車といったカテゴリに基づいて学習が行われるため，未知のカテゴリを分類することはできない．
そのため，セグメンテーションの学習データがそろっていない物体に対してビデオ・セグメンテーションを行う場合には，VOSが適している．

VOSは推定の過程でどれだけ人の手が加わるかに基づいて，Automatic VOS（以下，AVOS），Semi-automatic VOS（以下，SVOS），Interactive VOS（以下，IVOS）に分類できる\cite{garcia2018survey}．
\begin{description}
   \item[・Automatic Video Object Segmentation]\mbox{}\\
   教師なしVOSとも呼ばれる．人の手を加えることなく，動画内で支配的な物体を検出し自動的にセグメンテーションを行う．動画のみを入力とするため，どの物体に注目するかを指定することができない．
   \item[・Semi-automatic Video object Segmentation]\mbox{}\\
   半教師ありVOSとも呼ばれる．動画の最初の画像のみに対して，抽出したい部分がセグメンテーションされた初期マスク画像を，動画に加えて入力とする．以後の動画について，初期のマスク画像の情報をもとに自動的にセグメンテーションを行う．
   セグメンテーションしたい部分を四角の枠で囲った画像を初期の入力として与える手法やも存在する．
   \label{SVOS}
   \item[・Interactive Video object Segmentation]\mbox{}\\
   教師ありVOSとも呼ばれる．各画像の推定過程において人が補助を加えながらセグメンテーションを行う．セグメンテーションの精度が高くなる一方で，長時間人によって監視する必要がある．
\end{description}
%動画内において注目する部分を指定したく，また，人の手をできるだけかけない方法としては，SVOSが適している．

SVOSの関する研究では，2016年以前はセグメンテーション全般に使えるような特徴を学習し，それらの一般的な特徴と与えられた初期マスクから得た特徴を元に，後の画像についてセグメンテーションしていくことが主流であった．
次に，伝播型と呼ばれる，一時刻前のセグメンテーションで得られた画像を元に次のマスクを推定する手法が主流となった．しかし，伝播型のセグメンテーション手法は，推定が進むにつれてずれが蓄積されていくことや，注目している物体が他の物体によって遮られること（以下，オクルージョン）による推定精度の低下が課題となった\cite{garcia2018survey}．

それに対して，2020年頃からSpace-Time Memory (STM)\cite{oh2019video}を用いて以前に計算されたセグメンテーション情報を格納しておき，注目している物体の時間的な変化を学習するモデルが主流となった．しかし，STMを用いた手法ではビデオが長尺になるほどメモリが不足するという問題が生じた．
そこで，セグメンテーション情報を捉える際に，時間軸が異なるメモリを複数併用することで，長尺でもメモリ不足に陥らない手法が提案されている\cite{cheng2022xmem}．

\subsubsection{オプティカルフロー}\label{subsec:optical_flow}

オプティカルフロー（Optical Flow）推定とは，連続した2フレームの画像間で対応する点のフロー（画像座標上の変位）を推定するタスクである．
例えば，時刻$t=k$において画像座標$\mathbf{p}_k={(u_k,v_k)}^\mathrm{T}$に存在していた画素
が，時刻$t=k+1$にける画像座標${\mathbf{p}'}_{k+1}={( {u'}_{k+1}, {v'}_{k+1})}^\mathrm{T}$に存在する画素
に対応する場合に，この画素$\mathbf{p}_k$のフロー$\mathbf{f}(\mathbf{p}_k)=(f_{(u)},f_{(v)})^\mathrm{T}$は，
\begin{equation}
    \mathbf{f}(\mathbf{p}_k)=
\left(
\begin{array}{c}
     f_{(u)}  \\
     f_{(v)} 
\end{array}
\right)
=
\left(
\begin{array}{c}
     u'-u  \\
     v'-v
\end{array}
\right)
={\mathbf{p'}}_{k+1}-\mathbf{p}_k
\end{equation}
と定義される．

オプティカルフロー推定は疎なオプティカルフロー（Sparse Optical Flow）と密なオプティカルフロー（Dense Optical Flow）に大別される．
前者は抽出された特徴点についてのみフローを取得するのに対し，後者は全画素に対してフローを推定する．
密なオプティカルフローの出力として得られるフローのマップを補間することで，任意の画像座標のフローを推定することが可能になる．

オプティカルフローは従来，人の手によって設定された損失関数をもとに，最適化問題として解かれていた
\cite{chen2016full,horn1981determining,zach2007duality}ため，カメラや物体の推定速度や外的条件が変化した場合の安定性（以下，ロバスト性）に問題があった．
近年は,
2フレーム間から得た特徴を比較する畳み込みニューラルネットワーク(Convolutional Neural Network 以下，CNN)を導入したFlowNet\cite{dosovitskiy2015flownet}を皮切りに，深層学習に基づいて直接オプティカルフローを推定する手法が主流となっており，速度・精度・ロバスト性の面で従来手法を上回っている．

\subsubsection{単眼深度推定}\label{subsec:depth_estimation}

コンピュータービジョンの分野における深度推定とは，画像の各画素に対応する三次元空間上の点とカメラとのカメラ中心軸方向の距離（以下，深度）を推定する技術である．
絶対座標$\mathbf{m}^{(0)}$をもつ三次元空間上の点の時刻$t=k$における深度は，$m^{(k)}_{(z)}$にあたる．
深度推定の中でも，単眼カメラで捉えた画像一枚のみを入力とする単眼深度推定は，幾何学的に解くことが不可能な問題である．

そこで近年，深層学習を用いて単眼カメラで撮影された画像のみから深度推定を行う手法が盛んに研究されてきた．
CNNを用いた初期の深度推定手法はEigenら\cite{eigen2014depth}によって提案された．
単眼深度推定の教師あり学習は教師データとして画像に対応する密な（画素単位の）深度マップが必要であり，データセット作成の難しさから学習に必要な多くのデータセットを作れないことが課題であった．
そこでステレオカメラで得た画像ペアを学習データとする半教師あり手法\cite{garg2016unsupervised}や，単眼カメラの動画を学習データとする教師なし手法\cite{zhou2017unsupervised}に注目が集まっている．
半教師あり学習の中には，教師あり学習と同程度の精度を誇るアーキテクチャ\cite{Guizilini_2020_CVPR}も存在する．

しかし，深層学習をベースとした単眼深度推定は，学習するデータによって結果に差が出やすいという問題点がある．
特に，学習していない環境のデータに対しては深度のスケールを推定することができす，相対的な深度（以下，相対深度）のみしか求めることができない．
この場合，数点の絶対深度を既知として与えることで，単眼深度推定によって得られた相対深度を絶対深度に変えるスケールの調整を行う必要がある\cite{ranftl2020towards}．
また，学習済みのデータに対しても，物体までの距離に対して1割ほどの推定誤差がでてしまう\cite{fu2018deep}など，精度も課題である．

\subsection{Visual SLAM}\label{subsec:visual_SLAM}

移動するセンサに対して，センサの位置姿勢と周辺の環境地図構築を同時推定する問題は，Simultaneous Localization and Mapping (以下，SLAM)と呼ばれている．
SLAMは自律移動するあらゆる機械の制御に欠かすことができない技術であり，自動運転やドローンの自律飛行を目的として，近年活発に研究されている\cite{tomono正裕2018slam}．
SLAMは以下のような手順で行われる．
まず，各時刻にでのランドマーク（センサでとらえた特徴的な物体）とセンサの自己位置の幾何学的条件を複数取得する．
これらの条件と，以前の時刻で推定された自己位置・周辺の環境地図から得られる条件を組み合わせ，現在の時刻の自己位置の推定と周辺の環境地図の更新・拡大を同時に行う．
この作業を逐次的に繰り返すことで，各時刻のセンサの自己位置とセンサ周辺の環境地図を推定することが可能になる．

SLAMの課題として，周辺環境が一様な場合にでは，情報量が足りずSLAMを構築できない，もしくは精度が低下する「退化」と呼ばれる現象が挙げられる\cite{tomono正裕2020}．退化の例として，白い壁が続く廊下でSLAMを行うと，レーザーまたはカメラからの入力だけではセンサが静止しているか移動しているかを判断できず，精度が低下する．
これを克服するために，角速度を測定するジャイロセンサや，加速度センサ，全球測位衛星システム（Global Navigation Satellite System． 以下，GNSS）といった複数のセンサを融合させる手法も提案されている．

SLAMは用いる入力センサの違いにより，LiDAR SLAMとVisual SLAMに大別される\cite{tomono正裕2020}．
Visual SLAMについては続く項で取り上げる．

LiDAR SLAMとは，LiDARと呼ばれるレーザスキャナを入力センサとしたSLAMである．
LiDARは距離を直接計測して点群データを得られるため，安定したSLAMを実現しやすい．
また，暗所でも用いることができる．
その一方で，空間解像度が低く疎な点群しか得られないこと，時間解像度が低いため，動的物体が周辺環境に含まれる際に，それらを上手く捉えられないことなどが問題である．
また，機器自体が高価であることも普及しづらい原因となっている\cite{tomono正裕2020}．

\begin{figure}[b]
    \centering
    \includegraphics[width=0.5\linewidth]{images/SfM_2aspects.jpg}
    \caption[Structure from Motionで解く三次元復元と視点推定の問題]{Structure from Motionで解く三次元復元（左）と視点推定（右）の問題（\cite{sfm_shalaby2017algorithms}より）}
    \label{fig:sfm_figure}
\end{figure}

\subsubsection{Visual SLAMの特徴}\label{subsec:character_of_visual_SLAM}
カメラを入力センサとして用いたSLAMはVisual SLAMと呼ばれている．
カメラは，普及率が高く計測も容易なため，比較的扱いやすいセンサといえる．
そのほかにも利点として，空間解像度・時間解像度が高いことがあげられる．
空間解像度が高いことにより微小な部分まで撮影対象をとらえることができ，時間解像度が高いことにより物体の高速な運動をとらえることが可能である．

その一方で，画像自体には深度情報が含まれないため，Structure from Motion（以下，SfM）と呼ばれる三次元復元手法を組み込む必要があり，これがVisual SLAMを不安定化させる原因となっている\cite{tomono正裕2020}．
SfM（図\ref{fig:sfm_figure}）とはある対象を複数の視点から撮影した画像群を入力として，画像間の特徴点を対応づけることで撮影対象物の三次元復元と各視点の姿勢を同時推定する技術である\cite{中村恭之2017-08-05}．
SfMでは，まず入力画像から抽出された特徴点を画像間で対応付け，それぞれの特徴点について次の方程式を立てる．

例えば，フレーム$t=k$上の特徴点$i$について，特徴点の画像座標（投影平面上右向きに$u$軸，下向きに$v$軸をとる）
$\mathbf{p}_{k,i}={(u_{k,i} , v_{k,i})}^\mathrm{T}$と，
特徴点の三次元座標（座標系$t=k$上）
$\mathbf{m}^{(k)}_{k,i}={(x^{(k)}_{k,i} , y^{(k)}_{k,i} , z^{(k)}_{k,i})}^\mathrm{T}$を用いて，
\begin{equation}\label{eq:point2pro}
	\mathbf{p}_{k,i}
	=
	\boldsymbol{\pi}_{\text{Cam},k}
	\left(\mathbf{m}^{(k)}_{k,i}
	\right)
\end{equation}
と表現できる．ただし，フレーム$t=k$でのカメラの投影関数を次の$\boldsymbol{\pi}_{\text{Cam},k}$とした．
ここで，$f_{x,k}$，$f_{y,k}$はフレーム$t=k$での焦点距離，$(c_{x,k},c_{y,k})$はフレーム$t=k$での光学中心の画像座標を指す．
入力画像がすべて同じカメラで撮影されている場合，この投影関数$\boldsymbol{\pi}_{\text{Cam},k}$は全フレームで共通のものとなる．
\begin{equation}\label{eq:projection}
	\boldsymbol{\pi}_{\text{Cam},k}
	\left(
	\begin{array}{c}
	X\\Y\\Z
	\end{array}
	\right)
	=
	\left(
	\begin{array}{c}
		\frac{X}{Z}f_{x,k}+c_{x,k}  \\ \\  \frac{Y}{Z}f_{y,k}+c_{y,k} 
	\end{array}
	\right)
\end{equation}
このとき，$t=k$の自己位置$\mathbf{R}^{(0)}_{k}, \, \mathbf{t}^{(0)}_{k} $を用いて，
フレーム$t=k$上の特徴点$i$の三次元同次絶対座標は
\begin{equation}\label{eq:henkan}
    \tilde{\mathbf{m}}^{(0)}_{k,i}=
    \left(\begin{array}{cc}
\mathbf{R}^{(0)}_{k}  &  \mathbf{t}^{(0)}_{k}  \\
 \mathbf{0} & 1 
\end{array}\right)
    \tilde{\mathbf{m}}^{(k)}_{k,i}
   % = \mathbf{X}^{(0)}_{k}\,\mathbf{m}^{(k)}_{k,i} 
\end{equation}
より得られる．
これがすべてのフレームにおいて同時に成立することから，方程式の未知数，すなわち，各フレームの自己位置
$ \mathbf{R}^{(0)}_{k}, \, \mathbf{t}^{(0)}_{(k)}$
と各特徴点の三次元座標$\mathbf{m}^{(0)}_{k}$を推定する．

Visual SLAMでは以前のフレームと現在のフレームを用いてSfMを行うことにより，現在のフレームの自己位置および周辺環境の三次元地図を逐次的に更新していく．

\subsubsection{動的環境下でのVisual SLAM}\label{subsec:dynamic_visual_SLAM}
ここ数十年でVisual SLAMは急激な発展を遂げてきたが，ほとんどの技術は周辺環境が変化しないという想定で成り立つものであった．
Visual SLAMはSfMを基礎技術として用いており，SfMは撮影対象物が剛体かつ静止していることを前提としているからである．
そのため，動的物体を含む環境に対しては，特徴点の誤対応や，オクルージョンにより精度が急激に落ちるという問題点があった\cite{tan2013robust}．
これを克服したのが，動的物体と静止背景を個別に扱い，動的物体の影響を排除する方法である\cite{saputra2018visual}．
動的物体の影響を排除する方法には幾何学的条件を用いるといった手法も挙げられるが，\ref{subsec:video_segmentation}節で説明したセグメンテーションを用いる手法は具体的に以下の手順を踏む．
\begin{enumerate}
 \item 入力された連続画像に対してセグメンテーションを行うことで，動的物体の画素領域と静止背景の画素領域に分類する（図\ref{fig:segmentation_semantic}，\ref{fig:segmentation_mask}）．
 \item 各時刻の画像から検出された特徴点のうち，静止背景の画素領域のみから特徴点を抽出し（図\ref{fig:extract}），それらの静止している特徴点を用いて従来のVisual SLAMを適用する．これにより，カメラ姿勢推定と静止背景のみの三次元環境地図の構築を行う．
 \label{tejun2}
 \item 上の手順で得られたカメラ姿勢とセグメンテーションされた動的物体の深度推定結果などを統合し，動的物体の周辺環境地図内での三次元復元および追跡を行う．
\end{enumerate}

\begin{figure}[h]
\centering
	\begin{minipage}[t]{0.45\hsize}
		\centering
		\includegraphics[width=0.8\linewidth]{images/segmentation_semantic.png}
		\subcaption{セグメンテーションによって得られたラベル付き領域}
		\label{fig:segmentation_semantic}
 	\end{minipage}
	\begin{minipage}[t]{0.45\hsize}
		\centering
		\includegraphics[width=0.8\linewidth]{images/segmentation_mask.png}
		\subcaption{動的物体（人）の領域を抽出したマスク}
		\label{fig:segmentation_mask}
	\end{minipage}
	\begin{minipage}[t]{0.45\hsize}
		\centering
		\includegraphics[width=0.8\linewidth]{images/segmentation_extracted.png}
		\subcaption{マスクにより分離された背景の特徴点（青）と動的物体上の特徴点（赤）の例}
		\label{fig:extract}
		
	\end{minipage}
 \caption[動的物体のセグメンテーション,
 特徴点分離の例]{動的物体のセグメンテーション,
 	特徴点分離の例(\cite{liu2021rds}より）}
 \label{fig:segmentation}
\end{figure}

動的物体を含む環境下でのVisual SLAMに関する研究の多くは，上記の手順\ref{tejun2}までのカメラの姿勢推定および静的な背景の復元に重きを置いてきた．
動的物体の部分についても，動的物体が直線や円錐曲線上を動くと仮定して追跡する\cite{alcantarilla2012combining,avidan1999trajectory}など，動きに対して何かしらの仮定が設けられている．
動的物体の複雑な動きを捉えることや，高精度な位置推定・表面の微小な変位推定を行おうとしたVisual SLAMの研究は少ない．


