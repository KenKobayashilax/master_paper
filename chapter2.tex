\section{既往研究の整理}\label{sec:researches}
本章では，本研究に関係する既往研究を整理する．\par
\subsection{Structure from MotionとMulti View Stereo}\label{subsec:Sfm_and_MVS}
ここでは画像群のみからの環境の三次元復元において，現在の主要な手法であるStructure from Motion（以下，SfM）とMulti View Stereo（以下，MVS）について述べる．
この手法は異なる視点から撮影された画像群を入力として，まずはSfMを用いて物体の疎な三次元点群と各画像の三次元位置と姿勢を出力する．
SfMで得られた出力と画像を入力とし，MVSで点群を密にしていく．\par

SfMは，移動するカメラから得られる画像から，画像に写った対象物の幾何学的形状とカメラの動きを同時に復元する手法である\cite{sfm_JP}.
SfMは特徴点抽出・マッチングとバンドル調整という二つの段階に大別される．特徴点抽出ではSIFT\cite{SIFT}やORB\cite{ORB}といった特徴量記述子を用いて，エッジやコーナーといった特徴的な画像状の点を抽出する．
近年は深層学習を用いた特徴点記述子の研究も多く進められている．
こうして得られた特徴点を，対応する特徴量を用いて画像間で対応させていくことで，共通した三次元点を映し出す画像座標の対応を得ることができる．この際，RANSAC（Random Sample Consensus）\cite{RANSAC}を適用し，拘束条件（共線条件もしくはエピポーラ拘束）を満たさない誤対応点を排除する等の措置が取られる．
画像座標の対応が得られたら，カメラの位置・姿勢と対応点の三次元座標をバンドル調整を用いて計算する．
バンドル調整では，対応点の三次元座標を再び画像に投影した際の画像座標の誤差（再投影誤差）が最小になるように，非線形最小二乗法によってカメラの位置・姿勢と対応点の三次元座標の同時最適化が行われる．\par

MVSでは，三枚以上の画像を同時利用して画像間の対応に基づく密な三次元点群の生成を行う．
MVS手法の多くはマッチングコスト（類似度）の計算，周辺領域におけるコストの集約，視差の計算と最適化，視差の改善という4つのステップで行われ\cite{mvs}，計算された視差に基づいて画像上の各点に対応する三次元点群が密に生成される．\par

以上のように，SfMやMVSは画像間の対応関係に基づいて三次元復元を行っているため，特徴点が取りづらい場合や特徴点間の対応関係がつけづらい場合に失敗するという課題がある．
前者については，ピンボケやカメラの手ブレ，テクスチャの少ない被写体（白い壁が続く廊下など）といった状況があげられる．
後者については，鏡面反射や透過といった見る方向によって見え方が異なる場合，物体が他の物体に隠れてしまう場合（以下，オクルージョン）などがあげられる．
このような状況では，SfMによるカメラ位置・姿勢の推定に大きな誤差が含まれたり，最悪の場合姿勢推定が不可能になる．
また，特徴点間の対応関係がつけづらい部分が復元されないということも起こる．\par

\subsection{Differentiable Rendering}\label{subsec:Differentiable Rendering}
画像群を入力として，撮影されていない新たな視点からの画像レンダリング（以下，Novel View Synthesis）という文脈で，Differentiable Renderingという手法が近年活発に研究されている．\par

1990年代以前，Novel View Synthesisでは三次元モデルの作成とモデルからの画像のレンダリングは分離されていた．
そのため，モデル作成時に画像情報が欠損するという理由から，視覚的再現性が下がるという課題があった．
そこで，レンダリング時に，（三次元モデルに加え）画像情報を直接使用するImage-based Renderingという手法群が研究されてきた．
Image-based Renderingは明示的な三次元モデルをどれだけ利用するかに応じて大別される\cite{image_based_rendering}．
明示的な三次元モデルを用いないPlenoptic Modeling\cite{pleptonic_modeling}やLight Field\cite{light_field}，特徴点の対応関係といった画像間の位置対応を利用するview interpoltion\cite{view_interpolation}，画像の深度情報を用いて明示的に三次元情報を表現する3D Warping\cite{3d_warping}といった手法が主である．\par

三次元モデルの勾配を，画像のみの比較を通じてend-to-endで計算，伝播できるようにImage-based Renderingが拡張されたレンダリング手法群がDifferentiable Renderingである．
Differentiable Renderingではレンダリングした画像と撮影画像を直接比較するため，画像間の対応関係を用いる必要がなく，鏡面反射や透過といった見る方向によって見え方が異なる場合に対しても頑強である．
Differentiable Renderingは，Image-based Renderingの強みである視覚的再現性の高さと，幾何学的な条件を考慮できるという三次元モデルを使用した手法の強みを持ち合わせている一方で，レンダリングシステムが複雑であることやモデル構築に多くの写真が必要であるという欠点もある\cite{differentiable_rendering}．
Differentiable Renderingは三次元の表現方法に応じて，メッシュベース，ボクセルベース，点群ベース，陰関数ベースの手法に大別される．\par

中でも，陰関数ベースの手法であるNeural Radiance Field（以下，Nerf）\cite{nerf}は，新視点画像生成における視覚的再現性の高さから、コンピュータービジョン分野で多くの研究がなされている．
Nerfは三次元空間をradiance fieldと呼ばれる場で表現し，radiance fieldをMulti Layer Perceptron（以下、MLP）で学習・表現した手法である．
このネットワークは，三次元位置$(x,y,z)$と視線方向$(\theta, \phi)$を入力として，色$c$と密度$\sigma$を出力する．
レンダリングをする際は，光線上の点をサンプリングし，密度$\sigma$を重みとして色を積分していくことで画素値が定まる．
色が視線方向の関数となることで反射といった視線方向による見え方の違いを表現できるとともに，密度に基づいて色を積分するため光の透過も表現できる．
Nerfは陰関数ベースの手法であるため，連続的な表現が可能であり，少ないメモリで複雑な表現が可能という強みがある．
一方，光線上の点のサンプリングは計算コストが高いことに加え，空間全体を学習する必要があるため，学習やレンダリング時に多くの時間がかかる（学習は数時間，レンダリングは0.1FPS以下）という問題点がある\cite{3dgs}．
ボクセルやハッシュグリッドを用いて離散的にradiance fieldを表現することで学習とレンダリングを高速化する手法もある\cite{plenoxels}\cite{instant-ngp}ものの，レンダリング速度は10FPS程と，リアルタイムレンダリングは難しい．
2023年に発表された3DGS\cite{3dgs}は、三次元のガウシアンの集まりとしてradiance fieldを表現することで、高い視覚的再現性を保ちながらリアルタイムでのレンダリング（>100FPS）を可能にした。
詳細は次節で述べる。

\subsection{3D Gaussian Splatting}\label{subsec:3dgs_researches} 
本節では3DGSの特徴を述べたのち、ピンボケに適用可能な3DGS手法および、カメラ標定要素が不正確な場合に修正を行う3DGS手法について紹介する。

\subsubsection{3D Gaussian Splattingの特徴}\label{subsubsec:feature_of_3dgs}
radiance fieldを異方性を持った三次元のガウシアンの集まりとして表現した手法が3DGS\cite{3dgs}である。
光線上のガウシアンの値を積分するためランダムなサンプリングを必要とせず、Nerfと比べてモデル作成速度およびレンダリング速度が早いことが特徴である。
また、ガウシアンというexplcitなradiance fieldの表現により、MLPで空間を表現するNerfと比べ解釈可能性の高さも有している。\par

3DGSはシーンを撮影した画像群と各画像のカメラ標定要素、疎な三次元点群を入力として、シーンを表現するradiance fieldを生成する。
そのプロセスは、初期化、レンダリング、更新からなる（図：\ref{fig:3dgs_overview}）。
ガウシアンの初期化を行ったのち、入力画像とレンダリング画像を比較し、誤差に基づいてモデルの更新を繰り返し行う。
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\linewidth]{images/chapter2/3dgs_overview.png}
  \caption[3DGSの概略図]{3DGSの概略図。SfMで出力された三次元点群でガウシアンを初期化し（右上）、レンダリングとモデルの更新を繰り返し行う。}
  \label{fig:3dgs_overview}
\end{figure}

\subsubsection*{初期化}
3DGSは入力としてシーンを撮影した画像群と、それらにSfMを適用した結果であるカメラ標定要素・三次元点群用いる。
カメラ標定要素は固定された値として扱い、疎な点群によりガウシアンを初期化する。
各ガウシアンは三次元位置$x$（平均）、共分散行列$\Sigma$、不透明度$\alpha$、色$c(\theta, \phi)$を持っている。
色は\cite{plenoxels}\cite{instant-ngp}と同様に球面調和関数を用いることで、視線方向$(\theta, \phi)$の関数としてコンパクトなradiance fieldの表現を可能としている。\par

\subsubsection*{レンダリング}
ガウシアンで表現された三次元空間から画像をレンダリングする際は、まずガウシアンを画像上に投影し、次にガウシアンの色をカメラからの深度と不透明度$\alpha$に基づき積分していく。
なお、三次元のガウシアンを二次元のガウシアンに投影する際、三次元の共分散行列$\Sigma$は以下のように二次元の共分散行列$\Sigma^{\prime}$に変換される。
\begin{equation}
  \Sigma^{\prime}=J W \Sigma W^T J^T
\end{equation}
ここで、$W$はカメラの変換行列、$J$は投影変換を表すアフィン近似のヤコビ行列である。この時、レンダリングする視点により二次元のガウシアンの色は一意に定まる。
そして、ピクセルごとにカメラから見て手前にあるガウシアンから順に、不透明度$\alpha$を重みとして色を足し合わせていく。
不透明度の合計が一定値を超えた段階でピクセルの値が決定する。\par

\subsubsection*{更新}
モデルの更新は、ガウシアンのパラメタ最適化とガウシアンの密度調整を繰り返し行うことで実現される。
パラメタ最適化では、まずレンダリング画像と入力画像の画像類似度を計算する。
画像間の損失$L$はL1損失とD-SSIM損失\cite{d-ssim}を組み合わせ、以下のようにあらわされる。
\begin{equation}
  L = (1 - \lambda)L_1 + {\lambda}L_{D-SSIM}
\end{equation}
ここで、$\lambda$はD-SSIMの重みであり、\cite{3dgs}およびこれ以降に紹介される3DGS手法では$\lambda = 0.2$が使用されている。
この損失を最小化するよう確率的勾配降下法によってガウシアンのパラメタが更新される。\par

ガウシアンの密度調整は、初期化の時点では疎なガウシアンを、複製・分割することで適切に空間を表現できるよう密にしていく工程である。
まず、レンダリング画像と入力画像を比べた際に、誤差が閾値より大きい部分のガウシアンに対して適用される。
次に、そのガウシアンの大きさ（共分散行列の行列式$|\Sigma|$）に応じて複製または分割される（図：\ref{fig:density_control}）。
大きさが閾値より小さい場合は、足りていない部分をカバーする必要があるため、同じサイズのガウシアンが複製される。
大きい場合は、ガウシアンを形状を再現できるよう適切なサイズにするため、小さいサイズのガウシアンへと分割される。

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{images/chapter2/density_control.png}
    \caption[ガウシアンの密度調整]{ガウシアンの密度調整（\cite{3dgs}より作成）。形状（黒枠）に対してガウシアンが小さい場合（左上）は複製、大きい場合（左下）は分割される。}
    \label{fig:density_control}
\end{figure}

\subsubsection{Defocusing 3D Gaussian Splatting}\label{subsec:defocusing_3dgs}
ピンボケした不鮮明な入力画像から鮮明な画像をレンダリングする新視点画像生成手法は、Nerfの拡張として研究されてきた。
Deblur-Nerf\cite{deblur-nerf}は、ピンボケ表現をMLPで学習することにより、初めて全焦点画像を必要とせずピンボケ画像のみから鮮明な画像の生成を可能にした。
他にも、DP-Nerf\cite{dp-nerf}やSharp-Nerf\cite{sharp-nerf}といった深層学習を用いたピンボケの表現手法が提案された。
DoF-Nerf\cite{dof-nerf}は、カメラレンズの口径とピントが完全に合っている深度（以下、focal distance）を学習パラメタとした。
この学習パラメタを用いて、ピンボケの大きさをカメラからの深度に応じて変化させ、カメラの原理を考慮したピンボケ表現を導入した。
しかし、この手法はモデル作成時にピンボケ画像に加えて全焦点画像を必要とするという課題がある。
以上に述べたNerfベースの手法は、ピンボケ画像から鮮明な新視点画像の生成を実現したが、レンダリング速度が遅いというNerf由来の課題が依然としてある。\par

3DGSよるピンボケ画像からの新視点画像生成の研究も進められている。
Deblurring 3D Gaussian Splatting\cite{Deblurring3dgs}は、MLPを用いて各ガウシアンの変形量を推定することで、ピンボケを表現した。
また、BAGS\cite{BAGS}は、RGBD画像をニューラルネットワークの入力とし、画像座標上でのピンボケの大きさを推定した。
両手法ともにカメラからガウシアンまでの深度が考慮されているが、カメラからの深度とピンボケの大きさの関係は明示的に考慮されていない。
そのため、ガウシアンがピンボケに過剰に適合するよう学習され、ピンボケ量の推定が失敗する場合がある。

% \subsection{用語および記号の定義}\label{subsec:definition}

% ここでは本論文で用いる用語と記号を定義する．

% まず，本論文における自己位置とは，センサの３次元座標$\mathbf{t}={
% 	(t_{(x)} , 
% 	t_{(y)} ,
% 	t_{(z)})
% }^\mathrm{T}\in\mathbb{R}^3$に加えて，姿勢の回転方向を含む，計6つの自由度を持った情報である．センサの姿勢は，$z,y,x$軸周りの回転である３つのオイラー角$\alpha, \beta, \gamma$の組み合わせ以外にも，次の回転行列$\mathbf{R}$で表すこともでき，本論文では後者を採用する．
% \begin{equation}
% 	\mathbf{R}=
% 	\mathbf{R}_{z}(\alpha) \mathbf{R}_{y}(\beta) \mathbf{R}_{x}(\gamma)=
% 	\left[\begin{array}{ccc}\cos \alpha & -\sin \alpha & 0 \\ \sin \alpha & \cos \alpha & 0 \\ 0 & 0 & 1\end{array}\right]
% 	\left[\begin{array}{ccc}\cos \beta & 0 & \sin \beta \\ 0 & 1 & 0 \\ -\sin \beta & 0 & \cos \beta\end{array}\right]
% 	\left[\begin{array}{ccc}1 & 0 & 0 \\ 0 & \cos \gamma & -\sin \gamma \\ 0 & \sin \gamma & \cos \gamma\end{array}\right]
% \end{equation}
% \par
% 続いて，空間座標系を自己位置$(\mathbf{t},  \mathbf{R})$に応じて定義する．時刻$t=l$の自己位置によって定まる空間座標系（以後，座標系$t=l$と呼ぶ）上での時刻$t=k$の自己位置を$(\mathbf{t}^{(l)}_k,  \mathbf{R}^{(l)}_k)$として，
% \begin{equation}
% 	\mathbf{t}^{(k)}_k=\left(\begin{array}{c}
% 		0\\0\\0
% 	\end{array}\right), \,  
% 	\mathbf{R}^{(k)}_k=\left[\begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{array}\right]
% \end{equation}
% となるように空間座標系を定義する．以上の条件のもと，センサがカメラである場合には特に，投影平面の右方向に$x$軸，下方向に$y$軸，投影平面を垂直に奥向きに貫く方向を$z$軸とする．
% この時，座標系$t=k$上の点
% $\mathbf{m}^{(k)}={(x^{(k)} , y^{(k)}, z^{(k)})}^\mathrm{T}$は
% ,同次座標表現された位置ベクトル$\tilde{\mathbf{m}}^{(k)}={(x^{(k)} , y^{(k)}, z^{(k)},1)}^\mathrm{T}$を用いて座標系$t=l$上で
% \begin{equation}\label{eq:trans}
% 	\tilde{\mathbf{m}}^{(l)}
% 	=
% 	\left(\begin{array}{c}
% 		x^{(l)}\\ y^{(l)}\\ z^{(l)} \\ 1
% 	\end{array}
% 	\right)
% 	=
% 	\left[\begin{array}{cc}
% 		\mathbf{R}^{(l)}_k & \mathbf{t}^{(l)}_k \\ \mathbf{0} & 1
% 	\end{array}
% 	\right]
% 	\tilde{\mathbf{m}}^{(k)}
% \end{equation}
% と表現できる．
% 以後，特に座標系$t=0$を絶対座標系とし，それに従い$\mathbf{m}^{(0)}$を絶対座標，$\mathbf{R}^{(0)}_k,\, \mathbf{t}^{(0)}_k$をまとめて絶対自己位置または単に自己位置と呼ぶ．
% %また，座標系$t=l$での時刻$t=k$の自己位置$(\mathbf{t}^{(l)}_k,  \mathbf{R}^{(l)}_k)$を座標系$t=l$に対する時刻$t=k$の相対自己位置と呼ぶ．
% \par

% \subsection{要素技術}\label{subsec:techniques}

% 本研究で提案手法，および比較手法を構築するために用いられる画像処理・推定技術を紹介する．
% 具体的には，ビデオ・セグメンテーション，オプティカルフローの推定，単眼深度推定である．
% これらの技術は近年の機械学習の著しい進歩を背景に，深層学習に基づく推定手法が精度・速度の面で高い成果を上げている．

% \subsubsection{ビデオ・セグメンテーション}\label{subsec:video_segmentation}

% セグメンテーションとは画像に映り込んだ物体の画素領域を分離する技術であり，コンピュータービジョン分野におけるトピックの一つである．
% セグメンテーションの出力の一つとして得られる，注目する物体と他の部分において画素値が異なる画像は，マスク画像と呼ばれる．
% その中でもビデオ・セグメンテーションとは，動画を入力としてセグメンテーションを行うタスクである．
% 一枚の画像をセグメンテーションするイメージ・セグメンテーションと異なり，ビデオ・セグメンテーションはフレーム間の物体の対応関係が与えられるという点で，画像内での物体の追跡を包含している．
% 物体同士の対応については，コーナーやエッジ，輪郭といった部分的な特徴を持った点（以下，特徴点）を抽出し，それらをもとに探索・追跡が行われ，物体が対応付けられる．

% ビデオ・セグメンテーションはVideo Object Segmentation(以下，VOS)とVideo Semantic Segmentation(以下，VSS)に分かれる．
% VOSとは動画内の何かしらの物体をカテゴリの指定なく抽出するタスクである．
% セグメンテーションされた物体が何であるかという物体認識は含まれない．
% VSSとはあらかじめ定義されていたカテゴリ分類に基づいて物体を抽出するタスクである．
% 歩行者や車といったカテゴリに基づいて学習が行われるため，未知のカテゴリを分類することはできない．
% そのため，セグメンテーションの学習データがそろっていない物体に対してビデオ・セグメンテーションを行う場合には，VOSが適している．

% VOSは推定の過程でどれだけ人の手が加わるかに基づいて，Automatic VOS（以下，AVOS），Semi-automatic VOS（以下，SVOS），Interactive VOS（以下，IVOS）に分類できる\cite{garcia2018survey}．
% \begin{description}
%    \item[・Automatic Video Object Segmentation]\mbox{}\\
%    教師なしVOSとも呼ばれる．人の手を加えることなく，動画内で支配的な物体を検出し自動的にセグメンテーションを行う．動画のみを入力とするため，どの物体に注目するかを指定することができない．
%    \item[・Semi-automatic Video object Segmentation]\mbox{}\\
%    半教師ありVOSとも呼ばれる．動画の最初の画像のみに対して，抽出したい部分がセグメンテーションされた初期マスク画像を，動画に加えて入力とする．以後の動画について，初期のマスク画像の情報をもとに自動的にセグメンテーションを行う．
%    セグメンテーションしたい部分を四角の枠で囲った画像を初期の入力として与える手法やも存在する．
%    \label{SVOS}
%    \item[・Interactive Video object Segmentation]\mbox{}\\
%    教師ありVOSとも呼ばれる．各画像の推定過程において人が補助を加えながらセグメンテーションを行う．セグメンテーションの精度が高くなる一方で，長時間人によって監視する必要がある．
% \end{description}
% %動画内において注目する部分を指定したく，また，人の手をできるだけかけない方法としては，SVOSが適している．

% SVOSの関する研究では，2016年以前はセグメンテーション全般に使えるような特徴を学習し，それらの一般的な特徴と与えられた初期マスクから得た特徴を元に，後の画像についてセグメンテーションしていくことが主流であった．
% 次に，伝播型と呼ばれる，一時刻前のセグメンテーションで得られた画像を元に次のマスクを推定する手法が主流となった．しかし，伝播型のセグメンテーション手法は，推定が進むにつれてずれが蓄積されていくことや，注目している物体が他の物体によって遮られること（以下，オクルージョン）による推定精度の低下が課題となった\cite{garcia2018survey}．

% それに対して，2020年頃からSpace-Time Memory (STM)\cite{oh2019video}を用いて以前に計算されたセグメンテーション情報を格納しておき，注目している物体の時間的な変化を学習するモデルが主流となった．しかし，STMを用いた手法ではビデオが長尺になるほどメモリが不足するという問題が生じた．
% そこで，セグメンテーション情報を捉える際に，時間軸が異なるメモリを複数併用することで，長尺でもメモリ不足に陥らない手法が提案されている\cite{cheng2022xmem}．

% \subsubsection{オプティカルフロー}\label{subsec:optical_flow}

% オプティカルフロー（Optical Flow）推定とは，連続した2フレームの画像間で対応する点のフロー（画像座標上の変位）を推定するタスクである．
% 例えば，時刻$t=k$において画像座標$\mathbf{p}_k={(u_k,v_k)}^\mathrm{T}$に存在していた画素
% が，時刻$t=k+1$にける画像座標${\mathbf{p}'}_{k+1}={( {u'}_{k+1}, {v'}_{k+1})}^\mathrm{T}$に存在する画素
% に対応する場合に，この画素$\mathbf{p}_k$のフロー$\mathbf{f}(\mathbf{p}_k)=(f_{(u)},f_{(v)})^\mathrm{T}$は，
% \begin{equation}
%     \mathbf{f}(\mathbf{p}_k)=
% \left(
% \begin{array}{c}
%      f_{(u)}  \\
%      f_{(v)} 
% \end{array}
% \right)
% =
% \left(
% \begin{array}{c}
%      u'-u  \\
%      v'-v
% \end{array}
% \right)
% ={\mathbf{p'}}_{k+1}-\mathbf{p}_k
% \end{equation}
% と定義される．

% オプティカルフロー推定は疎なオプティカルフロー（Sparse Optical Flow）と密なオプティカルフロー（Dense Optical Flow）に大別される．
% 前者は抽出された特徴点についてのみフローを取得するのに対し，後者は全画素に対してフローを推定する．
% 密なオプティカルフローの出力として得られるフローのマップを補間することで，任意の画像座標のフローを推定することが可能になる．

% オプティカルフローは従来，人の手によって設定された損失関数をもとに，最適化問題として解かれていた
% \cite{chen2016full,horn1981determining,zach2007duality}ため，カメラや物体の推定速度や外的条件が変化した場合の安定性（以下，ロバスト性）に問題があった．
% 近年は,
% 2フレーム間から得た特徴を比較する畳み込みニューラルネットワーク(Convolutional Neural Network 以下，CNN)を導入したFlowNet\cite{dosovitskiy2015flownet}を皮切りに，深層学習に基づいて直接オプティカルフローを推定する手法が主流となっており，速度・精度・ロバスト性の面で従来手法を上回っている．

% \subsubsection{単眼深度推定}\label{subsec:depth_estimation}

% コンピュータービジョンの分野における深度推定とは，画像の各画素に対応する三次元空間上の点とカメラとのカメラ中心軸方向の距離（以下，深度）を推定する技術である．
% 絶対座標$\mathbf{m}^{(0)}$をもつ三次元空間上の点の時刻$t=k$における深度は，$m^{(k)}_{(z)}$にあたる．
% 深度推定の中でも，単眼カメラで捉えた画像一枚のみを入力とする単眼深度推定は，幾何学的に解くことが不可能な問題である．

% そこで近年，深層学習を用いて単眼カメラで撮影された画像のみから深度推定を行う手法が盛んに研究されてきた．
% CNNを用いた初期の深度推定手法はEigenら\cite{eigen2014depth}によって提案された．
% 単眼深度推定の教師あり学習は教師データとして画像に対応する密な（画素単位の）深度マップが必要であり，データセット作成の難しさから学習に必要な多くのデータセットを作れないことが課題であった．
% そこでステレオカメラで得た画像ペアを学習データとする半教師あり手法\cite{garg2016unsupervised}や，単眼カメラの動画を学習データとする教師なし手法\cite{zhou2017unsupervised}に注目が集まっている．
% 半教師あり学習の中には，教師あり学習と同程度の精度を誇るアーキテクチャ\cite{Guizilini_2020_CVPR}も存在する．

% しかし，深層学習をベースとした単眼深度推定は，学習するデータによって結果に差が出やすいという問題点がある．
% 特に，学習していない環境のデータに対しては深度のスケールを推定することができす，相対的な深度（以下，相対深度）のみしか求めることができない．
% この場合，数点の絶対深度を既知として与えることで，単眼深度推定によって得られた相対深度を絶対深度に変えるスケールの調整を行う必要がある\cite{ranftl2020towards}．
% また，学習済みのデータに対しても，物体までの距離に対して1割ほどの推定誤差がでてしまう\cite{fu2018deep}など，精度も課題である．

% \subsection{Visual SLAM}\label{subsec:visual_SLAM}

% 移動するセンサに対して，センサの位置姿勢と周辺の環境地図構築を同時推定する問題は，Simultaneous Localization and Mapping (以下，SLAM)と呼ばれている．
% SLAMは自律移動するあらゆる機械の制御に欠かすことができない技術であり，自動運転やドローンの自律飛行を目的として，近年活発に研究されている\cite{tomono正裕2018slam}．
% SLAMは以下のような手順で行われる．
% まず，各時刻にでのランドマーク（センサでとらえた特徴的な物体）とセンサの自己位置の幾何学的条件を複数取得する．
% これらの条件と，以前の時刻で推定された自己位置・周辺の環境地図から得られる条件を組み合わせ，現在の時刻の自己位置の推定と周辺の環境地図の更新・拡大を同時に行う．
% この作業を逐次的に繰り返すことで，各時刻のセンサの自己位置とセンサ周辺の環境地図を推定することが可能になる．

% SLAMの課題として，周辺環境が一様な場合にでは，情報量が足りずSLAMを構築できない，もしくは精度が低下する「退化」と呼ばれる現象が挙げられる\cite{tomono正裕2020}．退化の例として，白い壁が続く廊下でSLAMを行うと，レーザーまたはカメラからの入力だけではセンサが静止しているか移動しているかを判断できず，精度が低下する．
% これを克服するために，角速度を測定するジャイロセンサや，加速度センサ，全球測位衛星システム（Global Navigation Satellite System． 以下，GNSS）といった複数のセンサを融合させる手法も提案されている．

% SLAMは用いる入力センサの違いにより，LiDAR SLAMとVisual SLAMに大別される\cite{tomono正裕2020}．
% Visual SLAMについては続く項で取り上げる．

% LiDAR SLAMとは，LiDARと呼ばれるレーザスキャナを入力センサとしたSLAMである．
% LiDARは距離を直接計測して点群データを得られるため，安定したSLAMを実現しやすい．
% また，暗所でも用いることができる．
% その一方で，空間解像度が低く疎な点群しか得られないこと，時間解像度が低いため，動的物体が周辺環境に含まれる際に，それらを上手く捉えられないことなどが問題である．
% また，機器自体が高価であることも普及しづらい原因となっている\cite{tomono正裕2020}．

% \begin{figure}[b]
%     \centering
%     \includegraphics[width=0.5\linewidth]{images/SfM_2aspects.jpg}
%     \caption[Structure from Motionで解く三次元復元と視点推定の問題]{Structure from Motionで解く三次元復元（左）と視点推定（右）の問題（\cite{sfm_shalaby2017algorithms}より）}
%     \label{fig:sfm_figure}
% \end{figure}

% \subsubsection{Visual SLAMの特徴}\label{subsec:character_of_visual_SLAM}
% カメラを入力センサとして用いたSLAMはVisual SLAMと呼ばれている．
% カメラは，普及率が高く計測も容易なため，比較的扱いやすいセンサといえる．
% そのほかにも利点として，空間解像度・時間解像度が高いことがあげられる．
% 空間解像度が高いことにより微小な部分まで撮影対象をとらえることができ，時間解像度が高いことにより物体の高速な運動をとらえることが可能である．

% その一方で，画像自体には深度情報が含まれないため，Structure from Motion（以下，SfM）と呼ばれる三次元復元手法を組み込む必要があり，これがVisual SLAMを不安定化させる原因となっている\cite{tomono正裕2020}．
% SfM（図\ref{fig:sfm_figure}）とはある対象を複数の視点から撮影した画像群を入力として，画像間の特徴点を対応づけることで撮影対象物の三次元復元と各視点の姿勢を同時推定する技術である\cite{中村恭之2017-08-05}．
% SfMでは，まず入力画像から抽出された特徴点を画像間で対応付け，それぞれの特徴点について次の方程式を立てる．

% 例えば，フレーム$t=k$上の特徴点$i$について，特徴点の画像座標（投影平面上右向きに$u$軸，下向きに$v$軸をとる）
% $\mathbf{p}_{k,i}={(u_{k,i} , v_{k,i})}^\mathrm{T}$と，
% 特徴点の三次元座標（座標系$t=k$上）
% $\mathbf{m}^{(k)}_{k,i}={(x^{(k)}_{k,i} , y^{(k)}_{k,i} , z^{(k)}_{k,i})}^\mathrm{T}$を用いて，
% \begin{equation}\label{eq:point2pro}
% 	\mathbf{p}_{k,i}
% 	=
% 	\boldsymbol{\pi}_{\text{Cam},k}
% 	\left(\mathbf{m}^{(k)}_{k,i}
% 	\right)
% \end{equation}
% と表現できる．ただし，フレーム$t=k$でのカメラの投影関数を次の$\boldsymbol{\pi}_{\text{Cam},k}$とした．
% ここで，$f_{x,k}$，$f_{y,k}$はフレーム$t=k$での焦点距離，$(c_{x,k},c_{y,k})$はフレーム$t=k$での光学中心の画像座標を指す．
% 入力画像がすべて同じカメラで撮影されている場合，この投影関数$\boldsymbol{\pi}_{\text{Cam},k}$は全フレームで共通のものとなる．
% \begin{equation}\label{eq:projection}
% 	\boldsymbol{\pi}_{\text{Cam},k}
% 	\left(
% 	\begin{array}{c}
% 	X\\Y\\Z
% 	\end{array}
% 	\right)
% 	=
% 	\left(
% 	\begin{array}{c}
% 		\frac{X}{Z}f_{x,k}+c_{x,k}  \\ \\  \frac{Y}{Z}f_{y,k}+c_{y,k} 
% 	\end{array}
% 	\right)
% \end{equation}
% このとき，$t=k$の自己位置$\mathbf{R}^{(0)}_{k}, \, \mathbf{t}^{(0)}_{k} $を用いて，
% フレーム$t=k$上の特徴点$i$の三次元同次絶対座標は
% \begin{equation}\label{eq:henkan}
%     \tilde{\mathbf{m}}^{(0)}_{k,i}=
%     \left(\begin{array}{cc}
% \mathbf{R}^{(0)}_{k}  &  \mathbf{t}^{(0)}_{k}  \\
%  \mathbf{0} & 1 
% \end{array}\right)
%     \tilde{\mathbf{m}}^{(k)}_{k,i}
%    % = \mathbf{X}^{(0)}_{k}\,\mathbf{m}^{(k)}_{k,i} 
% \end{equation}
% より得られる．
% これがすべてのフレームにおいて同時に成立することから，方程式の未知数，すなわち，各フレームの自己位置
% $ \mathbf{R}^{(0)}_{k}, \, \mathbf{t}^{(0)}_{(k)}$
% と各特徴点の三次元座標$\mathbf{m}^{(0)}_{k}$を推定する．

% Visual SLAMでは以前のフレームと現在のフレームを用いてSfMを行うことにより，現在のフレームの自己位置および周辺環境の三次元地図を逐次的に更新していく．

% \subsubsection{動的環境下でのVisual SLAM}\label{subsec:dynamic_visual_SLAM}
% ここ数十年でVisual SLAMは急激な発展を遂げてきたが，ほとんどの技術は周辺環境が変化しないという想定で成り立つものであった．
% Visual SLAMはSfMを基礎技術として用いており，SfMは撮影対象物が剛体かつ静止していることを前提としているからである．
% そのため，動的物体を含む環境に対しては，特徴点の誤対応や，オクルージョンにより精度が急激に落ちるという問題点があった\cite{tan2013robust}．
% これを克服したのが，動的物体と静止背景を個別に扱い，動的物体の影響を排除する方法である\cite{saputra2018visual}．
% 動的物体の影響を排除する方法には幾何学的条件を用いるといった手法も挙げられるが，\ref{subsec:video_segmentation}節で説明したセグメンテーションを用いる手法は具体的に以下の手順を踏む．
% \begin{enumerate}
%  \item 入力された連続画像に対してセグメンテーションを行うことで，動的物体の画素領域と静止背景の画素領域に分類する（図\ref{fig:segmentation_semantic}，\ref{fig:segmentation_mask}）．
%  \item 各時刻の画像から検出された特徴点のうち，静止背景の画素領域のみから特徴点を抽出し（図\ref{fig:extract}），それらの静止している特徴点を用いて従来のVisual SLAMを適用する．これにより，カメラ姿勢推定と静止背景のみの三次元環境地図の構築を行う．
%  \label{tejun2}
%  \item 上の手順で得られたカメラ姿勢とセグメンテーションされた動的物体の深度推定結果などを統合し，動的物体の周辺環境地図内での三次元復元および追跡を行う．
% \end{enumerate}

% \begin{figure}[h]
% \centering
% 	\begin{minipage}[t]{0.45\hsize}
% 		\centering
% 		\includegraphics[width=0.8\linewidth]{images/segmentation_semantic.png}
% 		\subcaption{セグメンテーションによって得られたラベル付き領域}
% 		\label{fig:segmentation_semantic}
%  	\end{minipage}
% 	\begin{minipage}[t]{0.45\hsize}
% 		\centering
% 		\includegraphics[width=0.8\linewidth]{images/segmentation_mask.png}
% 		\subcaption{動的物体（人）の領域を抽出したマスク}
% 		\label{fig:segmentation_mask}
% 	\end{minipage}
% 	\begin{minipage}[t]{0.45\hsize}
% 		\centering
% 		\includegraphics[width=0.8\linewidth]{images/segmentation_extracted.png}
% 		\subcaption{マスクにより分離された背景の特徴点（青）と動的物体上の特徴点（赤）の例}
% 		\label{fig:extract}
		
% 	\end{minipage}
%  \caption[動的物体のセグメンテーション,
%  特徴点分離の例]{動的物体のセグメンテーション,
%  	特徴点分離の例(\cite{liu2021rds}より）}
%  \label{fig:segmentation}
% \end{figure}

% 動的物体を含む環境下でのVisual SLAMに関する研究の多くは，上記の手順\ref{tejun2}までのカメラの姿勢推定および静的な背景の復元に重きを置いてきた．
% 動的物体の部分についても，動的物体が直線や円錐曲線上を動くと仮定して追跡する\cite{alcantarilla2012combining,avidan1999trajectory}など，動きに対して何かしらの仮定が設けられている．
% 動的物体の複雑な動きを捉えることや，高精度な位置推定・表面の微小な変位推定を行おうとしたVisual SLAMの研究は少ない．


