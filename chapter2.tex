\section{既往研究の整理}\label{sec:researches}
本章では，本研究に関係する既往研究を整理する．\par
\subsection{Structure from MotionとMulti View Stereo}\label{subsec:Sfm_and_MVS}
ここでは画像群のみからの環境の三次元復元において，現在の主要な手法であるStructure from Motion（以下，SfM）とMulti View Stereo（以下，MVS）について述べる．
この手法は異なる視点から撮影された画像群を入力として，まずはSfMを用いて物体の疎な三次元点群と各画像の三次元位置と姿勢を出力する．
SfMで得られた出力と画像を入力とし，MVSで密な三次元モデルを生成する．\par

SfMは，被写体を複数の視点で撮影した画像群から，画像に写った対象物の幾何学的形状とカメラの動きを同時に復元する手法である\cite{sfm_JP}.
SfMは特徴点抽出・マッチングによる初期化とバンドル調整という二つの段階に大別される．特徴点抽出ではSIFT\cite{SIFT}やORB\cite{ORB}といった特徴量記述子を用いて，エッジやコーナーといった特徴的な画像状の点を抽出する．
近年は深層学習を用いた特徴点記述子の研究も多く進められている．
こうして得られた特徴点を，対応する特徴量を用いて画像間で対応させていくことで，共通した三次元点を映し出す画像座標の対応を得ることができる．この際，RANSAC（Random Sample Consensus）\cite{RANSAC}を適用し，拘束条件（共線条件もしくはエピポーラ拘束）を満たさない誤対応点を排除する等の措置が取られる．
画像座標の対応が得られたら，対応関係をもとに三次元点およびカメラ標定要素を初期化する．
初期化ののち，カメラの位置・姿勢と対応点の三次元座標をバンドル調整を用いて最適化する．
バンドル調整では，対応点の三次元座標を再び画像に投影した際の画像座標の誤差（再投影誤差）が最小になるように，非線形最小二乗法によってカメラの位置・姿勢と対応点の三次元座標の同時最適化が行われる（図\ref{fig:bundle_adjustment}）．\par
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\linewidth]{images/chapter2/bundle_adjustment.png}
  \caption[バンドル調整の概略図]{バンドル調整の概略図（\cite{bundle_adjustment}より）．特徴点（赤）と再投影点（青）の誤差を最小化するよう，三次元点（黒）とカメラ標定要素を最適化する．}
  \label{fig:bundle_adjustment}
\end{figure}
MVSでは，三枚以上の画像を同時利用して画像間の対応に基づく密な三次元モデルの生成を行う．
MVS手法の多くはマッチングコスト（類似度）の計算，周辺領域におけるコストの集約，視差の計算と最適化，視差の改善という4つのステップで行われ\cite{mvs}，計算された視差に対応する密な三次元モデルが生成される．\par

以上のように，SfMや多くのMVS手法では画像間の対応関係に基づいて三次元復元を行っているため，(1)特徴点が取りづらい場合や(2)特徴点間の対応関係の取得が困難な場合に失敗するという課題がある．
前者については，ピンボケやカメラの手ブレ，テクスチャの少ない被写体（白い壁が続く廊下など）といった状況が対応する．
後者については，鏡面反射や透過等の原因で見る方向によって見え方が異なる場合，また，物体が他の物体に隠れてしまう場合等が挙げられる．
このような状況では，SfMによるカメラ位置・姿勢の推定に大きな誤差が含まれ，最悪の場合姿勢推定が不可能になる．
また，特徴点間の対応関係の取得が困難な部分の復元が行われない場合がある．\par

\subsection{Differentiable Rendering}\label{subsec:Differentiable Rendering}
画像群を入力として，撮影されていない新たな視点からの画像レンダリング（以下，Novel View Synthesis）という文脈で，Differentiable Renderingという手法が近年活発に研究されている．\par

1990年代以前，Novel View Synthesisの分野では三次元モデルの作成とモデルからの画像のレンダリングは分離されていた．
そのため，モデル作成時に画像情報が欠損し，視覚的再現性が下がるという課題があった．
そこで，レンダリング時に，（三次元モデルに加え）画像情報を直接使用するImage-based Renderingという手法群が研究されてきた．
Image-based Renderingは明示的な三次元モデルをどれだけ利用するかに応じて大別される\cite{image_based_rendering}．
明示的な三次元モデルを用いないPlenoptic Modeling\cite{pleptonic_modeling}やLight Field\cite{light_field}，特徴点の対応関係といった画像間の位置対応を利用するview interpoltion\cite{view_interpolation}，画像の深度情報を用いて明示的に三次元情報を表現する3D Warping\cite{3d_warping}といった手法が主である．\par

三次元モデルの更新勾配を，画像のみの比較を通じてend-to-endで計算・伝播できるようにImage-based Renderingが拡張されたレンダリング手法群がDifferentiable Renderingである．
Differentiable Renderingではレンダリングした画像と撮影画像を直接比較するため，画像間の対応関係を用いる必要がなく，鏡面反射や透過といった見る方向によって見え方が異なる場合に対しても頑強である．
Differentiable Renderingは，Image-based Renderingの強みである視覚的再現性の高さと，幾何学的な条件を考慮できるという三次元モデルを使用した手法の強みを持ち合わせている\cite{differentiable_rendering}．
一方で，レンダリングシステムが複雑であることやモデル構築に多くの写真が必要であるという欠点もある\cite{differentiable_rendering}．
Differentiable Renderingは三次元の表現方法に応じて，メッシュベース，ボクセルベース，点群ベース，陰関数ベースの手法に大別される．\par

中でも，陰関数ベースの手法であるNeural Radiance Field（以下，Nerf）\cite{nerf}は，新視点画像生成における視覚的再現性の高さから，コンピュータービジョン分野で多くの研究がなされている．
Nerfは三次元空間をradiance fieldと呼ばれる場で表現し，radiance fieldをMulti Layer Perceptron（以下，MLP）で学習・表現した手法である（図\ref{fig:nerf}）．
このネットワークは，三次元位置$(x,y,z)$と視線方向$(\theta, \phi)$を入力として，色$c$と密度$\sigma$を出力する．
レンダリングをする際は，光線上の点をサンプリングし，密度$\sigma$を重みとして色を積分していくことで画素値が定まる．
色が視線方向の関数となることで視線方向による見え方の違いを表現できるとともに，密度に基づいて色を積分するため光の透過も表現できる．
Nerfは陰関数ベースの手法であるため，連続的な表現が可能であり，少ないメモリで複雑な表現が可能という強みがある．
一方，光線上の点のサンプリングは計算コストが高いことに加え，空間全体を学習する必要があるため，学習やレンダリング時に多くの時間がかかる（学習は数時間，レンダリングは$0.1 \mathrm{FPS}$以下）という問題点がある\cite{3dgs}．
ボクセルやハッシュグリッドを用いて離散的にradiance fieldを表現することで学習とレンダリングを高速化する手法もある\cite{plenoxels}\cite{instant-ngp}ものの，レンダリング速度は$10 \mathrm{FPS}$程と，リアルタイムのレンダリングは難しい．\par

2023年に発表された3DGS\cite{3dgs}は，三次元のガウシアンの集まりとしてradiance fieldを表現することで，高い視覚的再現性を保ちながらリアルタイムでのレンダリング（$>100 \mathrm{FPS}$）を可能にした．
詳細は次節で述べる．
\begin{figure}[h]
  \centering
  \includegraphics[width=0.65\linewidth]{images/chapter2/nerf.png}
  \caption[Nerfによる三次元空間表現]{Nerfによる三次元空間表現（\cite{nerf}より）．光線上の点をサンプリングし（左），各点でのMLPの出力を積分することでピクセル値が決まる（右）．}
  \label{fig:nerf}
\end{figure}

\subsection{3D Gaussian Splatting}\label{subsec:3dgs_researches} 
本節では3DGSの特徴を述べたのち，ピンボケに適用可能な3DGS手法および，カメラ標定要素が不正確な場合に修正を行う3DGS手法について紹介する．

\subsubsection{3D Gaussian Splattingの特徴}\label{subsubsec:feature_of_3dgs}
radiance fieldを異方性を持った三次元のガウシアンの集まりとして表現した手法が3DGS\cite{3dgs}である．
異方性を持つガウシアンを用いることで，ポリゴンや点群と比べて少ないパラメタで複雑かつ連続的な表現を実現している．
また，ガウシアンの持つ色を視線方向の関数とすることで，視線方向による見え方の違いを表現する．
光線上のガウシアンの値を積分するためランダムなサンプリングを必要とせず，Nerfと比べてモデル作成速度およびレンダリング速度が早いことが特徴である．
さらに，ガウシアンというexplcitなradiance fieldの表現により，MLPで空間を表現するNerfと比べ解釈可能性の高さも有している．\par

3DGSはシーンを撮影した画像群と各画像のカメラ標定要素，疎な三次元点群を入力として，シーンを表現するradiance fieldを生成する．
そのプロセスは，初期化，レンダリング，更新からなる（図\ref{fig:3dgs_overview}）．
ガウシアンの初期化を行ったのち，入力画像とレンダリング画像を比較し，誤差に基づいてモデルの更新を繰り返し行う．
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\linewidth]{images/chapter2/3dgs_overview.png}
  \caption[3DGSの概略図]{3DGSの概略図．SfM等で出力された三次元点群でガウシアンを初期化し（右上），レンダリングとモデルの更新を繰り返し行う．}
  \label{fig:3dgs_overview}
\end{figure}

\subsubsection*{初期化}
3DGSは入力としてシーンを撮影した画像群と，それらにSfM等を適用した結果であるカメラ標定要素・疎な三次元点群を用いる．
カメラ標定要素は固定された値として扱い，疎な点群によりガウシアンを初期化する．
各ガウシアンは三次元位置$x$（平均），共分散行列$\Sigma$，不透明度$\alpha$，色$c(\theta, \phi)$を持つ．
色は\cite{plenoxels}\cite{instant-ngp}と同様に球面調和関数を用いることで，視線方向$(\theta, \phi)$の関数としてコンパクトなradiance fieldの表現を可能としている．\par

\subsubsection*{レンダリング}
ガウシアンで表現された三次元空間から画像をレンダリングする際は，まずガウシアンを画像上に投影し，次にガウシアンの色をカメラからの深度と不透明度$\alpha$に基づき積分していく．
なお，三次元のガウシアンを二次元のガウシアンに投影する際，三次元の共分散行列$\Sigma$は以下のように二次元の共分散行列$\Sigma^{\prime}$に変換される．
\begin{equation}
  \Sigma^{\prime}=J W \Sigma W^T J^T
\end{equation}
ここで，$W$はカメラの変換行列，$J$は投影変換を表すアフィン近似のヤコビ行列である．この時，レンダリングする視点により二次元のガウシアンの色は一意に定まる．
そして，ピクセルごとにカメラから見て手前にあるガウシアンから順に，不透明度$\alpha$を重みとして色を足し合わせていく．
不透明度の合計が一定値を超えた段階でピクセルの値が決定する．\par

\subsubsection*{更新}
モデルの更新は，ガウシアンのパラメタ最適化とガウシアンの密度調整を繰り返し行うことで実現される．
パラメタ最適化では，まずレンダリング画像と入力画像の画像類似度を計算する．
画像間の損失$L$はL1損失とD-SSIM損失\cite{d-ssim}を組み合わせ，以下のようにあらわされる．
\begin{equation}
  L = (1 - \lambda)L_1 + {\lambda}L_{D-SSIM}
\end{equation}
ここで，$\lambda$はD-SSIMの重みであり，\cite{3dgs}およびこれ以降に紹介される3DGS手法では$\lambda = 0.2$が使用されている．
この損失を最小化するよう確率的勾配降下法によってガウシアンのパラメタが更新される．\par

ガウシアンの密度調整は，初期化の時点では疎なガウシアンを，複製・分割することで適切に空間を表現できるよう密にしていく工程である．
まず，レンダリング画像と入力画像を比べた際に，誤差が閾値より大きい部分のガウシアンに対して適用される．
次に，そのガウシアンの大きさ（共分散行列の行列式$|\Sigma|$）に応じて複製または分割される（図\ref{fig:density_control}）．
大きさが閾値より小さい場合は，足りていない部分をカバーする必要があるため，同じサイズのガウシアンが複製される．
大きい場合は，ガウシアンを形状を再現できるよう適切なサイズにするため，小さいサイズのガウシアンへと分割される．

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{images/chapter2/density_control.png}
    \caption[ガウシアンの密度調整]{ガウシアンの密度調整（\cite{3dgs}より作成）．形状（黒枠）に対してガウシアンが小さい場合（左上）は複製，大きい場合（左下）は分割される．}
    \label{fig:density_control}
\end{figure}

\subsubsection{Defocusing 3D Gaussian Splatting}\label{subsec:defocusing_3dgs}
ピンボケした不鮮明な入力画像から鮮明な画像をレンダリングする新視点画像生成手法は，Nerfの拡張として研究されてきた．
Deblur-Nerf\cite{deblur-nerf}は，ピンボケ表現をMLPで学習することにより，初めて全焦点画像を必要とせずピンボケ画像のみから鮮明な画像の生成を可能にした．
他にも，DP-Nerf\cite{dp-nerf}やSharp-Nerf\cite{sharp-nerf}といった深層学習を用いたピンボケの表現手法が提案された．
DoF-Nerf\cite{dof-nerf}は，カメラレンズの口径とピントが完全に合っている深度（以下，focal distance）を学習パラメタとした．
この学習パラメタを用いて，ピンボケの大きさをカメラからの深度に応じて変化させ，カメラの原理を考慮したピンボケ表現を導入した．
しかし，この手法はモデル作成時にピンボケ画像に加えて全焦点画像を必要とするという課題がある．
以上に述べたNerfベースの手法は，ピンボケ画像から鮮明な新視点画像の生成を実現したが，レンダリング速度が遅いというNerf由来の課題が依然としてある．\par

3DGSよるピンボケ画像からの新視点画像生成の研究も進められている．
Deblurring 3D Gaussian Splatting\cite{Deblurring3dgs}は，MLPを用いて各ガウシアンの変形量を推定することで，ピンボケを表現した．
また，BAGS\cite{BAGS}は，RGBD画像をニューラルネットワークの入力とし，画像座標上でのピンボケの大きさを推定した．
両手法ともにカメラからガウシアンまでの深度がニューラルネットワークへの入力という形で利用されている，カメラからの深度とピンボケの大きさの関係は明示的に考慮されていない．
そのため，ガウシアンがピンボケに過剰に適合するよう学習され，ピンボケ量の推定が失敗する場合がある．

\subsubsection{Defocusing 3D Gaussian Splatting}\label{subsec:defocusing_3dgs}
3DGSではカメラ標定要素を既知の入力として固定値で扱うため，SfMの失敗等によりカメラ標定要素に誤差がある場合は生成されるガウシアンにも誤差が生じる．
カメラ標定要素が不正確な場合，もしくは与えられていない場合に対して正しいカメラ標定要素の推定を行う手法も提案されている．
COLMAP-Free 3D GS\cite{colmapfree3dgaussiansplatting}では，動画を入力として，隣接画像間でのガウシアンの対応関係により相対的なカメラ標定要素を推定した．
具体的には，隣接画像では視点の変化は小さいという仮定のもと，$t=n$で推定されたガウシアンを$t=n+1$の入力画像に適合するように移動させ，その移動を逆変換するような相対カメラ標定要素を推定した．
入力画像が少なくSfMが失敗する場合への対応手法も提案されている\cite{instantsplat}.
しかし，この手法は深層学習を用いた外部モデル\cite{DUSt3R}でカメラの初期化を行うため，類似のシーンの学習経験がない場合に脆弱という課題がある．
また，iComMa\cite{iComMa}は正解のガウシアンが既に与えられている条件で，カメラ標定要素が未知の画像に対してカメラ標定要素の推定を行う手法である．
従来のレンダリング画像の画像類似度による損失だけでなく，微分可能な特徴点抽出ネットワーク\cite{LoFTR}を用いたマッチングの損失を組み込んだ．
これにより，マッチングの損失は標定要素に大きな誤差がある場合に有効であり，画像類似度による損失は小さい標定要素の誤差をさらに小さくする場合に有効だと示した．
上記のほかにも，カメラ標定要素の修正を伴う3DGS手法は提案されている\cite{ggrt}が，これらの手法は入力画像が鮮明な場合への適用に限られている．
入力画像が不鮮明な場合として，手ブレを伴う画像に対応する3DGS手法は提案されている\cite{BAD-Gaussians}.
この手法では，手ブレを複数のカメラ標定要素からレンダリングされた画像群の平均として表現するため，手法にカメラ標定要素推定が内包されている．
しかしながら，ピンボケ画像に対応可能でカメラ標定要素修正を伴う最適化を行う3DGS手法は管見の限りない．